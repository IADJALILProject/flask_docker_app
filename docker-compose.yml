version: '3.8'

services:
  # ðŸ”¹ Application Flask
  web:
    environment:
      - POSTGRES_USER=flaskuser
      - POSTGRES_PASSWORD=flaskpass
      - POSTGRES_DB=flaskdb
      - POSTGRES_HOST=db
      - POSTGRES_PORT=5432
    build: .
    ports:
      - "5000:5000"
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      - db
    restart: always

  # ðŸ”¹ Base de donnÃ©es PostgreSQL (partagÃ©e)
  db:
    image: postgres:15
    restart: always
    env_file:
      - .env
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # ðŸ”¹ Initialisation Airflow (exÃ©cutÃ©e une fois)
  airflow-init:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    entrypoint: >
      bash -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    restart: "no"
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
    depends_on:
      - db
    volumes:
      - ./airflow/dags:/opt/airflow/dags

  # ðŸ”¹ Webserver Airflow
  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - airflow-init
    env_file:
      - .env
    ports:
      - "8080:8080"
    environment:  
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/dbt                # <-- AjoutÃ©
      - ./dbt/.dbt:/root/.dbt
      - ./spark:/spark 
    command: airflow webserver
    

  # ðŸ”¹ Scheduler Airflow
  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - airflow-init
    env_file:
      - .env
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY}
    user: root
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/dbt                # <-- AjoutÃ©
      - ./dbt/.dbt:/root/.dbt
      - ./spark:/spark 
      - /var/run/docker.sock:/var/run/docker.sock  # <--- nÃ©cessaire !
    command: airflow scheduler
    
  dbt:
    container_name: flask_docker_app-dbt-1
    build:
      context: ./dbt
      dockerfile: Dockerfile
    volumes:
    - ./dbt:/dbt  # ton projet dbt
    - ./dbt/.dbt:/root/.dbt  # ton fichier profiles.yml montÃ© dans le bon dossier
    working_dir: /dbt/dbt_project
    ports:
      - "8082:8082"
    environment:
      - DBT_PROFILES_DIR=/root/.dbt
      - DBT_PROJECT_DIR=/dbt/dbt_project
      - DBT_USER=${POSTGRES_USER}
      - DBT_PASSWORD=${POSTGRES_PASSWORD}
      - DBT_HOST=db
      - DBT_PORT=5432
      - DBT_DATABASE=${POSTGRES_DB}
    entrypoint: ["tail", "-f", "/dev/null"]
    depends_on:
    - db
  
  spark:
    container_name: spark_app
    build:
      context: ./spark
      dockerfile: Dockerfile
    volumes:
      - ./airflow/dags/data:/opt/data
      - ./spark:/opt/spark_jobs
      - ./dbt:/dbt
      - ./dbt/.dbt:/root/.dbt
    working_dir: /opt/spark_jobs
    ports:
      - "4040:4040"
    command: tail -f /dev/null
    networks:
      - spark-net
      - default               # âœ… AjoutÃ©
    depends_on:              # âœ… AjoutÃ©
      - db
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}

  spark-master:
    image: bitnami/spark:latest
    container_name: spark_master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8088:8088"
    volumes:
      - ./spark:/opt/spark_jobs
      - ./airflow/dags/data:/opt/data
    working_dir: /opt/spark_jobs/jobs   
    command: tail -f /dev/null
    networks:
      - spark-net

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark_worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark_master:7077
    depends_on:
      - spark-master
    volumes:
      - ./spark:/opt/spark_jobs
      - ./airflow/dags/data:/opt/data
    working_dir: /opt/spark_jobs/jobs    
    command: tail -f /dev/null
    networks:
      - spark-net

  pgadmin:
    image: dpage/pgadmin4
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "8888:80"
    depends_on:
      - db


volumes:
  postgres_data:

networks:
  spark-net:
    driver: bridge
  default:                  # âœ… RÃ©seau partagÃ© implicite
    driver: bridge




