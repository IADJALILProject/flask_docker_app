from utils import get_spark_session, log_df_info
from pyspark.sql.functions import col, datediff, current_date
from pyspark.sql.types import FloatType
from pyspark.sql.functions import to_date

def main():
    # ğŸ“¦ Initialisation de la session Spark
    spark = get_spark_session("UserChurnAnalysis")

    # ğŸ“„ Lecture du fichier CSV
    df = spark.read.csv("/opt/data/users.csv", header=True, inferSchema=True)

    # ğŸ§¹ Nettoyage des donnÃ©es : suppression des lignes avec valeurs nulles
    df_clean = df.dropna(subset=["email", "last_login"]).dropDuplicates(["email"])

    # âœ… Convertir last_login en date si ce nâ€™est pas dÃ©jÃ  le cas
    df_clean = df_clean.withColumn("last_login", to_date(col("last_login")))

    log_df_info(df_clean, "DonnÃ©es pour churn analysis")

    # â±ï¸ Calcul du nombre de jours d'inactivitÃ©
    df_churn = df_clean.withColumn("days_since_last_login", datediff(current_date(), col("last_login")))

    # ğŸ“Š AgrÃ©gation par type dâ€™abonnement
    churn_stats = df_churn.groupBy("subscription_type").agg(
        {"days_since_last_login": "avg"}
    ).withColumnRenamed("avg(days_since_last_login)", "avg_days_inactive")

    # âœ… Cast pour compatibilitÃ© avec Pandas
    churn_stats = churn_stats.withColumn("avg_days_inactive", col("avg_days_inactive").cast(FloatType()))

    # ğŸ” Logging
    log_df_info(churn_stats, "Churn statistics par abonnement")

    # ğŸ’¾ Sauvegarde
    churn_stats.coalesce(1).write.mode("overwrite").parquet("/opt/data/churn_stats")
    churn_stats.toPandas().to_csv("/opt/data/churn_analysis.csv", index=False)

    print("âœ… churn_analysis.csv exportÃ© avec succÃ¨s.")

if __name__ == "__main__":
    main()


